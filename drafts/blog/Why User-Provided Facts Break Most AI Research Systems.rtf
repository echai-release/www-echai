{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Why User-Provided Facts Break Most AI Research Systems\
\
Most AI systems assume something that\'92s rarely true in the real world.\
\
That the information they start with is correct.\
And that it stays correct.\
\
In practice, users constantly add new facts mid-conversation:\
\
corrections\
\
missing context\
\
updated details\
\
edge cases the system didn\'92t know\
\
These inputs are incredibly valuable.\
They\'92re also risky.\
\
If you ignore them, your research becomes stale.\
If you blindly merge them, your system becomes inconsistent.\
\
That\'92s why user-provided facts need to be treated differently from scraped or preloaded data.\
\
They must be:\
\
detected explicitly\
\
validated structurally\
\
and merged deliberately\
\
Otherwise, you end up with systems that sound smart but quietly accumulate contradictions.\
\
The mistake most teams make is assuming fact ingestion is a model problem.\
\
It\'92s not.\
\
It\'92s a systems problem.}